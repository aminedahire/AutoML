{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aminedahire/AutoML/blob/main/Copie_de_TP4_automl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Ev-ywlvvyn"
      },
      "source": [
        "# TP: Machine Learning (SIA_3611)\n",
        "\n",
        "## TP4: AutoML (4h)\n",
        "\n",
        "by Guillaume Renton\n",
        "\n",
        "In previous TP, you have learned to use machine learning for different kind of tasks, from regression to clustering through classification. In this TP, you are going to use the earned knowledge on new datasets for regression and classification.\n",
        "\n",
        "You are going to use 2 new datasets in this TP. First one is california housing, whose target variable is the value of houses in california, expressed in hundred of thousand of dollars. For each house, a set of 9 features is available. There is a total of 20 060 data.\n",
        "\n",
        "Second one is MNIST, a very popular dataset for handwritten recognition and image classification. The original dataset is made of 60 000 training images of shape 28x28 of handwritten digits from 0 to 9, and 10 000 images for test dataset. For computaional time, you will work on a given random subset of MNIST made of 6000 images in train and 1000 images in test.  \n",
        "\n",
        "**Objectives :**\n",
        "- Apply your knowledge on new datasets\n",
        "- Tune models hyperparameters and explore metrics\n",
        "- Apply principal components analysis and understand its effects on both dataset\n",
        "- Understand and use Cross-Validation\n",
        "- Use AutoML to find interesting models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m9Z-NRuvvyq"
      },
      "source": [
        "### STEP 1 : Getting started with new datasets\n",
        "\n",
        "#### Substep 1 : Regression\n",
        "\n",
        "In first part of step 1, you will work on the regression problem with the dataset california housing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQQue8Zavvyr"
      },
      "source": [
        "**To do 1.1**\n",
        "\n",
        "Execute the following cell to load the california housing dataset and normalize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KyMV3QmJvvyr"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y = True)\n",
        "X = normalize(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9vynOO9vvys"
      },
      "source": [
        "**To code 1.2**\n",
        "\n",
        "Apply [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) and [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) methods and cross validate your results using 5 folders. For this, you can either use the function [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)(or any other method for cross validation in sklearn) or either compute yourself the cross validation. According to a relevant metric optimize both methods. For SGD you will optimize the value of alpha for both L2 and L1 penalty score. For SVR, you will optimize the kernel. Be careful with the metric if you use cross_val_score, the returned values are often negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bnk6A4xayoKh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "As7gTbKuvvys",
        "outputId": "39ace3bb-bfa2-4b39-a271-d6fb159b96e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for SGD with penalty l2: {'alpha': 0.1, 'penalty': 'l2'}\n",
            "Best cross-validation score: 1.3742208955790975\n",
            "\n",
            "Best parameters for SGD with penalty l1: {'alpha': 0.1, 'penalty': 'l1'}\n",
            "Best cross-validation score: 1.3714799514468143\n",
            "\n",
            "Best parameters for SVR: {'kernel': 'poly'}\n",
            "Best cross-validation score: 1.198046329410784\n"
          ]
        }
      ],
      "source": [
        "# Define the range of alpha values for SGD\n",
        "alphas = np.logspace(-6, -1, 6)\n",
        "\n",
        "# Define the models and parameters for SGD\n",
        "sgd_models_params = [\n",
        "    {'penalty': ['l2'], 'alpha': alphas},\n",
        "    {'penalty': ['l1'], 'alpha': alphas}\n",
        "]\n",
        "\n",
        "# Perform GridSearchCV for SGD\n",
        "for model_params in sgd_models_params:\n",
        "    sgd = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "    grid_sgd = GridSearchCV(sgd, model_params, cv=5, scoring='neg_mean_squared_error')\n",
        "    grid_sgd.fit(X, y)\n",
        "    print(f\"Best parameters for SGD with penalty {model_params['penalty'][0]}: {grid_sgd.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {-grid_sgd.best_score_}\\n\")\n",
        "\n",
        "# Define the kernels for SVR\n",
        "kernels = ['linear', 'rbf', 'sigmoid', 'poly']\n",
        "\n",
        "# Perform GridSearchCV for SVR\n",
        "svr = SVR()\n",
        "grid_svr = GridSearchCV(svr, {'kernel': kernels}, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_svr.fit(X, y)\n",
        "print(f\"Best parameters for SVR: {grid_svr.best_params_}\")\n",
        "print(f\"Best cross-validation score: {-grid_svr.best_score_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2z2UxWJvvyt"
      },
      "source": [
        "**Question 1**\n",
        "\n",
        "## According to your metric, which method obtain the best result ?\n",
        "\n",
        "According to the provided cross-validation scores (which are based on the Mean Squared Error), the Support Vector Regression (SVR) with a polynomial kernel achieves the best performance with the lowest score of approximately 1.198. It's important to note that lower values of Mean Squared Error (MSE) indicate better model performance for regression tasks. Therefore, SVR with a polynomial kernel is the best-performing model among the ones evaluated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZVYIqzdvvyt"
      },
      "source": [
        "**Question 2**\n",
        "\n",
        "## What is the interest of using cross validation in general ? Is it relevant in this particular case ?\n",
        "\n",
        "Cross-validation is essential for providing a reliable and unbiased performance estimate of different models, aiding in hyperparameter tuning, and ensuring a fair comparison across models, which is particularly important in scenarios like this with a moderately sized dataset and multiple models to evaluate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIUPLrujvvyu"
      },
      "source": [
        "**To code 1.3**\n",
        "\n",
        "Transform your data according to [principal component analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), and optimize the number of components according to the same metric than previously for both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zjjR5zdtvvyu"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "x6exdLOg4XDm",
        "outputId": "5f85f28d-ef5f-4056-86f6-7ea64659fcd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD with L2 penalty:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ed021b38f5ab>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# SGD with L2 penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SGD with L2 penalty:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mrandomized_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_l2_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'pca__n_components'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# SGD with L1 penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sgd_l2_pipe' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "\n",
        "n_components = np.arange(1, X.shape[1] + 1)\n",
        "# Shuffle the data and select a subset for faster computation\n",
        "X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n",
        "X_subset, y_subset = X_shuffled[:2000], y_shuffled[:2000]  # Adjust the size as needed\n",
        "\n",
        "# Define the pipelines and randomized search for each model because the gridsearch was taking too long\n",
        "def randomized_search(model, params):\n",
        "    search = RandomizedSearchCV(model, params, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
        "    search.fit(X_subset, y_subset)\n",
        "    print(f\"Best parameters: {search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {-search.best_score_}\\n\")\n",
        "\n",
        "gd_l2_pipe = Pipeline([\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('sgd', SGDRegressor(max_iter=1000, tol=1e-3, random_state=42, alpha=0.1, penalty='l2'))\n",
        "])\n",
        "\n",
        "# SGD with L1 penalty\n",
        "sgd_l1_pipe = Pipeline([\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('sgd', SGDRegressor(max_iter=1000, tol=1e-3, random_state=42, alpha=0.1, penalty='l1'))\n",
        "])\n",
        "\n",
        "svr_pipe = Pipeline([\n",
        "    ('pca', PCA(random_state=42)),\n",
        "    ('svr', SVR(kernel='poly'))\n",
        "])\n",
        "\n",
        "# SGD with L2 penalty\n",
        "print(\"SGD with L2 penalty:\")\n",
        "randomized_search(sgd_l2_pipe, {'pca__n_components': n_components})\n",
        "\n",
        "# SGD with L1 penalty\n",
        "print(\"SGD with L1 penalty:\")\n",
        "randomized_search(sgd_l1_pipe, {'pca__n_components': n_components})\n",
        "\n",
        "# SVR with polynomial kernel\n",
        "print(\"SVR with polynomial kernel:\")\n",
        "randomized_search(svr_pipe, {'pca__n_components': n_components})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE48J1govvyv"
      },
      "source": [
        "**Question 3**\n",
        "\n",
        "## What is the interest of Principal Component Analysis in general ? Is it relevant here ?\n",
        "\n",
        "**Interest of Principal Component Analysis (PCA):**\n",
        "\n",
        "PCA aids in reducing dimensionality, filtering out noise, and potentially improving model performance, which is beneficial for both computational efficiency and model optimization.\n",
        "\n",
        "**Relevance in This Scenario:**\n",
        "\n",
        "In this case, PCA proved useful in speeding up hyperparameter tuning without significantly degrading model performance. It also provided an additional parameter to optimize, contributing to the fine-tuning of the models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwy07pUyvvyv"
      },
      "source": [
        "#### Substep 2 : Classification\n",
        "\n",
        "**To do 1.4**\n",
        "\n",
        "Execute the following cells to load a subset of MNIST dataset. Since the dataset is already divided into training/test, we won't use cross validation this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gou3D5Nb6nd6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))  # Flatten the images\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))  # Flatten the images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnYUtr_Uvvyv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"data/mnist.pkl\", \"rb\") as f:\n",
        "    ((X_train, y_train), (X_test, y_test)) = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpD6KFbPvvyw"
      },
      "source": [
        "**To code 1.5**\n",
        "\n",
        "Compute classification on those images using a [KNN classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) and an [Adaboost classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). For each classifier, optimize the parameters according to a relevant metric. For the KNN classifier, you will optimize the number of neighbor while for the Adaboost classifier, you will optimize the base estimator along with the number of estimators (for the basis estimator, limit yourself to different depth of decision tree classifier).\n",
        "\n",
        "Also, for each model, compute the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4s6lsiovvyw"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Flatten the images and select a subset for faster computation\n",
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "X_test_flat = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "# Subset of the data for faster computation\n",
        "X_train_subset = X_train_flat[:6000]\n",
        "y_train_subset = y_train[:6000]\n",
        "X_test_subset = X_test_flat[:1000]\n",
        "y_test_subset = y_test[:1000]\n",
        "\n",
        "# KNN Classifier\n",
        "knn = KNeighborsClassifier()\n",
        "param_grid_knn = {'n_neighbors': np.arange(1, 5)}\n",
        "grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_knn.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "# AdaBoost Classifier\n",
        "ada = AdaBoostClassifier()\n",
        "param_grid_ada = {\n",
        "    'base_estimator': [DecisionTreeClassifier(max_depth=depth) for depth in [1, 2]],\n",
        "    'n_estimators': [50, 100]\n",
        "}\n",
        "grid_ada = GridSearchCV(ada, param_grid_ada, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_ada.fit(X_train_subset, y_train_subset)\n",
        "\n",
        "# Best parameters and accuracy for KNN\n",
        "print(\"Best parameters for KNN:\", grid_knn.best_params_)\n",
        "print(\"Best accuracy for KNN:\", grid_knn.best_score_)\n",
        "\n",
        "# Best parameters and accuracy for AdaBoost\n",
        "print(\"Best parameters for AdaBoost:\", grid_ada.best_params_)\n",
        "print(\"Best accuracy for AdaBoost:\", grid_ada.best_score_)\n",
        "\n",
        "# Confusion Matrix for KNN\n",
        "y_pred_knn = grid_knn.predict(X_test_subset)\n",
        "cm_knn = confusion_matrix(y_test_subset, y_pred_knn)\n",
        "print(\"Confusion Matrix for KNN:\\n\", cm_knn)\n",
        "\n",
        "# Confusion Matrix for AdaBoost\n",
        "y_pred_ada = grid_ada.predict(X_test_subset)\n",
        "cm_ada = confusion_matrix(y_test_subset, y_pred_ada)\n",
        "print(\"Confusion Matrix for AdaBoost:\\n\", cm_ada)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvSFSuejvvyw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuMKuQfOvvyx"
      },
      "source": [
        "**Question 4**\n",
        "\n",
        "## According to your metric, which method obtain the best results ?\n",
        "\n",
        "According to the accuracy metric, the KNN classifier obtains the best results with an accuracy of approximately 93.4%, while the AdaBoost classifier has a significantly lower accuracy of 61.6%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfK-pqMivvyx"
      },
      "source": [
        "**Question 5**\n",
        "\n",
        "## According to the confusion matrix, which class if the easiest to classify ? Which ones are the most difficult ? Which ones are the most confused with each other ?\n",
        "\n",
        "**Easiest to Classify:**\n",
        "\n",
        "    Class 1 seems to be the easiest to classify with the KNN classifier, as it has 126 true positives and no false positives or false negatives.\n",
        "    For the AdaBoost classifier, Class 1 also has relatively good performance, but not as perfect as in KNN.\n",
        "\n",
        "**Most Difficult to Classify:**\n",
        "\n",
        "    For KNN, Classes 2 and 9 seem more challenging. Class 2 has some misclassifications across several other classes, and Class 9 has several instances misclassified as Class 4.\n",
        "    For AdaBoost, many classes show poor performance, but Classes 2, 5, and 9 stand out. Class 2 has widespread misclassifications, Class 5 has a lot of instances misclassified as Class 9, and Class 9 has many instances misclassified as Class 4.\n",
        "\n",
        "**Most Confused With Each Other:**\n",
        "\n",
        "    For KNN, Class 9 is often confused with Class 4.\n",
        "    For AdaBoost, Class 5 is frequently confused with Class 9, and Class 9 is often confused with Class 4 and Class 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njAV8TU_vvyx"
      },
      "source": [
        "**Bonus**\n",
        "\n",
        "For the Adaboost classifier, explore other classifier as base estimators. What are the limitations about those estimators ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6uOB87WAjRk"
      },
      "outputs": [],
      "source": [
        "# Decision Tree Classifier\n",
        "dt_params = {\n",
        "    'base_estimator__max_depth': [1, 2, 3],\n",
        "    'n_estimators': [50, 100, 150]\n",
        "}\n",
        "dt = GridSearchCV(AdaBoostClassifier(base_estimator=DecisionTreeClassifier()), dt_params)\n",
        "dt.fit(X_train, y_train)\n",
        "print(f\"Best parameters for AdaBoost with Decision Tree: {dt.best_params_}\")\n",
        "print(f\"Best accuracy: {accuracy_score(y_test, dt.predict(X_test))}\")\n",
        "\n",
        "# Support Vector Classifier\n",
        "svc_params = {\n",
        "    'base_estimator__kernel': ['linear', 'rbf'],\n",
        "    'n_estimators': [50, 100, 150]\n",
        "}\n",
        "svc = GridSearchCV(AdaBoostClassifier(base_estimator=SVC(probability=True)), svc_params)\n",
        "svc.fit(X_train, y_train)\n",
        "print(f\"Best parameters for AdaBoost with SVC: {svc.best_params_}\")\n",
        "print(f\"Best accuracy: {accuracy_score(y_test, svc.predict(X_test))}\")\n",
        "\n",
        "# Logistic Regression\n",
        "lr_params = {\n",
        "    'base_estimator__C': [0.01, 0.1, 1, 10],\n",
        "    'n_estimators': [50, 100, 150]\n",
        "}\n",
        "lr = GridSearchCV(AdaBoostClassifier(base_estimator=LogisticRegression()), lr_params)\n",
        "lr.fit(X_train, y_train)\n",
        "print(f\"Best parameters for AdaBoost with Logistic Regression: {lr.best_params_}\")\n",
        "print(f\"Best accuracy: {accuracy_score(y_test, lr.predict(X_test))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEyrFuR4vvyy"
      },
      "source": [
        "**To code 1.6**\n",
        "\n",
        "Transform your data according to principal component analysis, and optimize the number of components according to the same metric than previously for each classifier.\n",
        "\n",
        "Once again, compute the confusion matrix for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj_WEkXivvyy"
      },
      "outputs": [],
      "source": [
        "knn_pipeline = Pipeline([\n",
        "    ('pca', PCA()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define parameter grid for KNN\n",
        "knn_param_grid = {\n",
        "    'pca__n_components': np.arange(1, X_train.shape[1] + 1, 10),\n",
        "    'knn__n_neighbors': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Perform grid search for KNN\n",
        "knn_random_search = RandomizedSearchCV(knn_pipeline, knn_param_grid, n_iter=10, cv=5, n_jobs=-1)\n",
        "knn_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print results for KNN\n",
        "print(f'Best parameters for KNN: {knn_random_search.best_params_}')\n",
        "print(f'Best accuracy for KNN: {knn_random_search.best_score_}')\n",
        "y_pred_knn = knn_random_search.predict(X_test)\n",
        "print('Confusion Matrix for KNN:')\n",
        "print(confusion_matrix(y_test, y_pred_knn))\n",
        "\n",
        "# Define a pipeline for AdaBoost\n",
        "adaboost_pipeline = Pipeline([\n",
        "    ('pca', PCA()),\n",
        "    ('adaboost', AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1)))\n",
        "])\n",
        "\n",
        "# Define parameter grid for AdaBoost\n",
        "adaboost_param_grid = {\n",
        "    'pca__n_components': np.arange(1, X_train.shape[1] + 1, 10),\n",
        "    'adaboost__n_estimators': [50, 100, 150]\n",
        "}\n",
        "\n",
        "# Perform grid search for AdaBoost\n",
        "adaboost_random_search = RandomizedSearchCV(adaboost_pipeline, adaboost_param_grid, n_iter=10, cv=5, n_jobs=-1)\n",
        "adaboost_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print results for AdaBoost\n",
        "print(f'Best parameters for AdaBoost: {adaboost_random_search.best_params_}')\n",
        "print(f'Best accuracy for AdaBoost: {adaboost_random_search.best_score_}')\n",
        "y_pred_adaboost = adaboost_random_search.predict(X_test)\n",
        "print('Confusion Matrix for AdaBoost:')\n",
        "print(confusion_matrix(y_test, y_pred_adaboost))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5JwyaITvvyz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR5qN6FCvvyz"
      },
      "source": [
        "**Question 6**\n",
        "\n",
        "Is the use of PCA relevant here ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7Gh2n0Lvvyz"
      },
      "source": [
        "**Question 7**\n",
        "\n",
        "Did your answers from question 5 changed with PCA ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwUepiegvvy0"
      },
      "source": [
        "### Step 2 : AutoML\n",
        "\n",
        "In this second section, we discuss on the utilisation of AutoMl tools, such as auto-sklearn.\n",
        "If you are using colab or don't have auto-sklearn installed, you may need to run the following cell at first in order to install auto-sklearn. This will require you to restart the runtime (a prompt will invite you to).\n",
        "\n",
        "Restarting the runtime will clear all your variables and imported libraries, so you will need to import them again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tHCaBAUi0_k"
      },
      "outputs": [],
      "source": [
        "sudo - apt install python 3.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOFqwOvCvvy0"
      },
      "outputs": [],
      "source": [
        "!pip install --force-reinstall scipy==1.6\n",
        "!pip install --force-reinstall auto-sklearn==0.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYt59rU3vvy0"
      },
      "source": [
        "**To do 2.1**\n",
        "\n",
        "Execute the following cells.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cANcwkFxvvy1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = fetch_california_housing(return_X_y = True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1pwavfBvvy1"
      },
      "outputs": [],
      "source": [
        "import autosklearn.regression\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import os, shutil\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "automl = autosklearn.regression.AutoSklearnRegressor(\n",
        "    include = {'regressor': [\"libsvm_svr\", \"sgd\"]},\n",
        "    time_left_for_this_task=120,\n",
        "    per_run_time_limit=30,\n",
        "    tmp_folder='/tmp/california_housing_tmp',\n",
        ")\n",
        "automl.fit(X_train, y_train, dataset_name='California_Housing')\n",
        "\n",
        "print(automl.leaderboard())\n",
        "\n",
        "y_pred = automl.predict(X_test, y_test)\n",
        "print(\"MSE = \", mean_squared_error(y_test, y_pred))\n",
        "print(\"MRE = \", mean_absolute_error(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BFft9_Hvvy2"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "pprint(automl.show_models(), indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azRI1ZyMvvy2"
      },
      "source": [
        "**Question 8**\n",
        "\n",
        "What are the evaluated models by autoML ?\n",
        "Which model obtain the best performance ?\n",
        "What are the parameters of the best model ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrK8zBd4vvy2"
      },
      "source": [
        "**To code 2.2**\n",
        "\n",
        "With the help of the previous code, use autoML for the classification task on MNIST, by limiting the exploration to KNN and Adaboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcJXJYTbvvy3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq07k-KHvvy3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz7Vu4uyvvy3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urBHPwC8vvy3"
      },
      "source": [
        "**Question 9**\n",
        "\n",
        "What are the evaluated models by autoML ?\n",
        "Which model obtain the best performance ?\n",
        "What are the parameters of the best model ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWppD8GZvvy3"
      },
      "source": [
        "### Bonus step\n",
        "\n",
        "As a bonus step, have fun and remove a maximum of constraints of your autoML model. Which model obtain the best performances ? Describe the parameters of this model. You can do it for either for regression or classification or both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0YdS2Qnvvy4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "MLenv",
      "language": "python",
      "name": "mlenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}